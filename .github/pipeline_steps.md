# ğŸ§ª Coffee Break Scraper & NLP Pipeline

Este documento describe el flujo general de trabajo para el proyecto de scraping y procesamiento de datos del pÃ³dcast *Coffee Break: SeÃ±al y Ruido*. El objetivo es extraer metadatos Ãºtiles, analizar el contenido con herramientas NLP y almacenar todo en una base de datos escalable.

---

## ğŸš€ Objetivo General

> Crear un sistema modular capaz de recorrer todos los episodios publicados, extraer metadatos (temas, tiempos, contertulios, enlaces), guardar versiones limpias y analizables del contenido, y permitir el procesamiento con NLP y diarizaciÃ³n cuando se disponga del audio.

---

## ğŸ§­ Etapas del Pipeline

### 1. ğŸ›°ï¸ Descubrimiento de episodios
- Scrapeo de la pÃ¡gina principal o Ã­ndice de episodios.
- ExtracciÃ³n de: `tÃ­tulo`, `URL`, `fecha`, `ID`.
- Guarda un archivo de control de episodios disponibles (`episodes_index.json`).

ğŸ”§ Archivo: `scraping/episodes_index_scraper.py`

---

### 2. ğŸ’¾ Descarga del HTML
- Descarga el HTML completo de cada pÃ¡gina de episodio.
- Almacena en: `data/raw_html/epXXX.html`
- Previene la dependencia futura del contenido online (versiÃ³n offline).

ğŸ”§ Archivo: `scraping/base_scraper.py`

---

### 3. ğŸ§¹ Parsing del HTML â†’ JSON estructurado
- Extrae de cada episodio:
  - `tÃ­tulo`, `fecha`, `cara A / B`, `temas`, `timestamps`
  - `contertulios`, `referencias`, `enlaces externos`
- Guarda como JSON limpio en `data/parsed_json/epXXX.json`
- Admite diferentes parsers para distintas estructuras de episodios.

ğŸ”§ Archivos:
- `scraping/episode_parser_v1.py` (episodios recientes)
- `scraping/episode_parser_legacy.py` (estructuras antiguas)

---

### 4. ğŸ›¢ï¸ Ingesta en base de datos
- Inserta los datos parseados en `episodes.db` (SQLite por defecto).
- Permite bÃºsquedas rÃ¡pidas, agregaciones, consultas.
- Limpieza de duplicados / control de versiones si hay cambios.

ğŸ”§ Archivo sugerido: `utils/db.py`

---

### 5. ğŸ”Š TranscripciÃ³n y diarizaciÃ³n (opcional)
- Para episodios con acceso al audio:
  - DiarizaciÃ³n con `pyannote.audio` (detecta quiÃ©n habla y cuÃ¡ndo)
  - TranscripciÃ³n con `whisper` (segmenta en texto por hablante)
- Resultado: texto con timestamp + nombre de hablante estimado.

ğŸ”§ Archivos:
- `nlp_pipeline/diarization.py`
- `nlp_pipeline/transcriber.py`

---

### 6. ğŸ§  AnÃ¡lisis NLP
- Procesamiento con `spaCy` (modelo espaÃ±ol `es_core_news_md`):
  - TokenizaciÃ³n
  - DetecciÃ³n de entidades (PERSON, LOC, ORG...)
  - ExtracciÃ³n de temas y relaciones semÃ¡nticas
- Opcional: vectorizaciÃ³n semÃ¡ntica (BERT, sentence-transformers)

ğŸ”§ Archivos:
- `nlp_pipeline/spacy_analysis.py`
- `notebooks/analysis_example.ipynb`

---

### 7. ğŸ§ª VisualizaciÃ³n y exploraciÃ³n de resultados
- Reportes interactivos, visuales y exportables.
- ExploraciÃ³n de episodios por tema, contertulio, aÃ±o...
- Soporte para grÃ¡ficos y exportaciÃ³n a `.csv`, `.json`, `.md` o `.html`

ğŸ““ Usar: `Jupyter`, `Altair`, `Plotly`, `Streamlit` *(en etapas futuras)*

---

### 8. ğŸ” OrquestaciÃ³n CLI
- Automatiza tareas con comandos como:
```bash
python cli.py download-index
python cli.py fetch-html --episode 505
python cli.py parse-episode --episode 505
python cli.py populate-db
```

ğŸ”§ Archivo: `cli.py`

---

## ğŸ“‚ Sugerencia de carpetas

```plaintext
coffee_scraper_project/
â”œâ”€â”€ scraping/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_html/
â”‚   â”œâ”€â”€ parsed_json/
â”‚   â””â”€â”€ audio_files/
â”œâ”€â”€ database/
â”œâ”€â”€ nlp_pipeline/
â”œâ”€â”€ notebooks/
â”œâ”€â”€ tests/
â”œâ”€â”€ cli.py
â”œâ”€â”€ config.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âœ¨ Posibles mejoras futuras
- ğŸ§  BÃºsqueda semÃ¡ntica por temas (FAISS / ChromaDB)
- ğŸ§ Descarga automÃ¡tica del audio y transcripciÃ³n masiva
- ğŸ“ˆ Dashboard con Streamlit
- ğŸ“š Entrenamiento de modelo personalizado para detecciÃ³n de tÃ³picos
- ğŸ“¤ ExportaciÃ³n pÃºblica de los datos parseados (API, CSV, etc.)

---

Â¿Listo para comenzar? Puedes empezar desde `episodes_index_scraper.py` o el parser del episodio que ya tenemos (505).

